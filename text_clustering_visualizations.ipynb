{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocessing text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load a English model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "#Personalized stopwords\n",
    "stopwords_extended= [\"eu\",\"ms\",\"august\",\"czech\", \"republic\",\"programme\",\"new\",\"aim\",\"russian\",\"response\",\"project\", \"ban\",\"use\",\"tonne\"]\n",
    "stopwords_extended.extend(stopwords)\n",
    "\n",
    "\n",
    "# Function removing HTML/XMLtags\n",
    "def striphtml(data):\n",
    "    pattern = re.compile(r'<.*?>')\n",
    "    return pattern.sub('', data)\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def text_preprocess_cleaning(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Generate lemmas and lower case and remove punctuation\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_punct]\n",
    "  \n",
    "    # Remove stopwords  characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords_extended ]\n",
    "    #\n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "    \n",
    "##########################   \n",
    "## TEXT PREPROCESSING \n",
    "##########################\n",
    "#### this function perform all steps needed for preprocessing text \n",
    "#### input : df= dataframe\n",
    "####         label = field's label of dataframe to apply text_preprocessing_cleaning\n",
    "####output : df= dataframe cleaned\n",
    "##########################    \n",
    "\n",
    "def preprocessing_process (df, label):\n",
    "    \n",
    "    \n",
    "    # 1. drop missing values\n",
    "    df.dropna(subset=[label], inplace=True)\n",
    "    \n",
    "    \n",
    "    # 2. Removing HTML/XMLtags\n",
    "    for text, row in df.iterrows():\n",
    "        df.loc[text, label] = striphtml(row[label])\n",
    "\n",
    "\n",
    "    # 3. Apply preprocess text\n",
    "    df[label] = df[label].apply(text_preprocess_cleaning)\n",
    "\n",
    "    # 3. drop empty row resulting from text preprocess\n",
    "    df = df.drop(df[df[label]==\"\"].index)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "source": [
    "# Evaluation methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def silhouette_score_(k_rng, tfidf_matrix):\n",
    "    \n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    sil = []\n",
    "    for k in k_rng :\n",
    "      kmeans = KMeans(n_clusters = k).fit(tfidf_matrix)\n",
    "      labels = kmeans.predict(tfidf_matrix)\n",
    "      sil.append(silhouette_score(tfidf_matrix, labels))\n",
    "    \n",
    "    print(len(sil))\n",
    "    #plot\n",
    "    plt.plot(k_rng,np.array(sil))\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Silhouette index\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def sse_scaler_(k_rng , tfidf_matrix):\n",
    "    sse_scaler  = []\n",
    "    for k in k_rng:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km.fit(tfidf_matrix)\n",
    "        km.predict(tfidf_matrix)\n",
    "        sse_scaler.append(km.inertia_)\n",
    "    #plot\n",
    "    plt.plot(k_rng,sse_scaler)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Sum of squared error\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "source": [
    "# NLP projects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "df= pd.read_csv(\"eu_projects.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "df_clean = preprocessing_process(df,'description')\n",
    "\n",
    "\n",
    "#define vectorizer parameters\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(df['description'])"
   ]
  },
  {
   "source": [
    "# Cluster model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['doc_cluster.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "###############################################\n",
    "# k-mean clustering\n",
    "###############################################\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "model = KMeans(num_clusters, random_state=123)\n",
    "\n",
    "model.fit(tfidf_matrix)\n",
    "\n",
    "\n",
    "#k_rng = range(2,8)\n",
    "#em.silhouette_score_(k_rng, tfidf_matrix)\n",
    "#em.sse_scaler_(k_rng, tfidf_matrix)\n",
    "\n",
    "\n",
    "joblib.dump(model,  'doc_cluster.pkl')"
   ]
  },
  {
   "source": [
    "# Cluster visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'df_clean'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bb22cf71a14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_group\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Project country\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Nid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'df_clean'"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "from dash.dependencies import Input, Output\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "##########################\n",
    "#### Analyse clustering\n",
    "##############\n",
    "\n",
    "model = joblib.load('doc_cluster.pkl')\n",
    "num_clusters = 5\n",
    "\n",
    "clusters=model.predict(tfidf_matrix)\n",
    "\n",
    "df_clean['cluster'] = pd.DataFrame(clusters)\n",
    "df_group =df_clean.groupby([\"cluster\",\"Project country\"])[\"Nid\"].nunique().to_frame()\n",
    "\n",
    "df_group.reset_index(inplace=True)\n",
    "df_group.columns = ['cluster', 'project','tot']\n",
    "\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (df_group[\"cluster\"] ==0),\n",
    "    (df_group[\"cluster\"] ==1),\n",
    "    (df_group[\"cluster\"] ==2),\n",
    "    (df_group[\"cluster\"] ==3),\n",
    "    (df_group[\"cluster\"] ==4)\n",
    "    ]\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [\"School Milk Program for Agriculture Development\", 'School Fruit and Vegetables Program for Agriculture Development', 'development & innovation', 'Market emergency','Fisheries, Maritime Affairs & environment']\n",
    "\n",
    "\n",
    "df_group['description_cluster'] = np.select(conditions, values)\n",
    "\n",
    "#assign colors to cluster\n",
    "colors=['pinkyl','tempo','magenta','amp','blues']\n",
    "\n",
    "\n",
    "\n",
    "app = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "# the style arguments for the sidebar.\n",
    "SIDEBAR_STYLE = {\n",
    "    'position': 'fixed',\n",
    "    'top': 0,\n",
    "    'left': 0,\n",
    "    'bottom': 0,\n",
    "    'width': '10%',\n",
    "    'padding': '20px 10px',\n",
    "    'background-color': '#f8f9fa'\n",
    "}\n",
    "\n",
    "# the style arguments for the main content page.\n",
    "CONTENT_STYLE = {\n",
    "    'margin-left': '5%',\n",
    "    'margin-right': '5%',\n",
    "    'padding': '20px 10p'\n",
    "}\n",
    "\n",
    "TEXT_STYLE = {\n",
    "    'textAlign': 'center',\n",
    "    'color': '#191970'\n",
    "}\n",
    "\n",
    "CARD_TEXT_STYLE = {\n",
    "    'textAlign': 'center',\n",
    "    'color': '#0074D9'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "content_first_row = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(\n",
    "             dcc.Dropdown(\n",
    "        id=\"dropdown\",\n",
    "        options=[\n",
    "            {'label': values[i], 'value': i}\n",
    "            for i in range(num_clusters)\n",
    "           ],\n",
    "        value=0,\n",
    "        clearable=False,\n",
    "            ),\n",
    "            md=12)\n",
    "    ]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph\", \"figure\"), \n",
    "    [Input(\"dropdown\", \"value\")])\n",
    "\n",
    "def display_topic_cluster(n):\n",
    "    \n",
    "    df_text_bow = nlp.tfidf_matrix.toarray()\n",
    "    bow_df = pd.DataFrame(df_text_bow)\n",
    "\n",
    "    # Map the column names to vocabulary \n",
    "    bow_df.columns = nlp.vectorizer.get_feature_names()\n",
    "    bow_df['cluster'] = pd.DataFrame(clusters)\n",
    "\n",
    "    word_freq = pd.DataFrame(bow_df[bow_df.cluster == n].sum().sort_values(ascending = False))\n",
    "    word_freq.reset_index(level=0, inplace=True)\n",
    "    word_freq.columns=['word','frequency']\n",
    "    \n",
    "    if n>0:\n",
    "        word_freq.drop(index=[0],inplace=True)\n",
    "        \n",
    "    fig = px.treemap(word_freq[0:30], path=[px.Constant(values[n]),'word'], values='frequency',\n",
    "                color='frequency', hover_data=['frequency'],\n",
    "                color_continuous_scale= colors[n])\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_second_row = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(\n",
    "            dcc.Graph(id='graph'), \n",
    "            md=12)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_zero_row = dbc.Row([\n",
    "    dbc.Col(\n",
    "        dbc.Card(\n",
    "            [\n",
    "\n",
    "                dbc.CardBody(\n",
    "                    [\n",
    "                        html.H4(id='dataset_title_1', children=['Text Clustering Analysis'], className='card-title',\n",
    "                                style=CARD_TEXT_STYLE),\n",
    "                         html.P(id='card_text_1', children=['Providing a meaningful visualization of the main topics represented in the projects funded by the EU'], style=CARD_TEXT_STYLE),\n",
    "                              \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                       \n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        md=12\n",
    "    )\n",
    "])\n",
    "\n",
    "import plotly\n",
    "\n",
    "\n",
    "\n",
    "content_third_row = dbc.Row(\n",
    "   [\n",
    "       dbc.Col(  \n",
    "         dcc.Graph(id=\"graph2\", \n",
    "                  figure = px.sunburst(df_group, path=['description_cluster','project'], values='tot',color='cluster')),md=6 ),\n",
    "       dbc.Col(  \n",
    "          dcc.Graph(id=\"graph3\", figure =px.scatter(df_group, x=\"project\", y=\"cluster\", size=\"tot\", color=\"description_cluster\").update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=1.2,\n",
    "    yanchor=\"bottom\",\n",
    "    xanchor=\"right\"\n",
    ")) ), md=6)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content = html.Div(\n",
    "    [\n",
    "       # html.H2('Text Analytics Dashboard', style=TEXT_STYLE),\n",
    "       # html.Hr(),\n",
    "        content_zero_row,\n",
    "        html.Br(),\n",
    "        content_first_row,\n",
    "        content_second_row,\n",
    "        content_third_row,\n",
    "        html.Hr()\n",
    "    ],\n",
    "    style=CONTENT_STYLE\n",
    ")\n",
    "\n",
    "\n",
    "app.layout = html.Div([ content])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}